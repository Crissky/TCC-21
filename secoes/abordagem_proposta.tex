\section{Abordagem proposta}
\label{sec:abordagem}
% Nessa seção devem ser apresentados os métodos utilizados no trabalho, ferramentas, dados e materiais.

Empresas que atuam em uma grande área do território nacional atendendo a um grande número de clientes podem ser ré de diversas ações judiciais iniciadas pelos consumidores que se sentiram lesados com os serviços prestados. Desse modo, essas numerosas ações judiciais podem ocorrer em diferentes locais ou comarcas, \enquote{\textit{território em que o juiz de primeiro grau irá exercer sua jurisdição e pode abranger um ou mais municípios}} \cite{cnj:comarca}, o que dificulta o acompanhamento dos processos forenses. Assim sendo, manter uma equipe jurídica do tamanho necessário para cuidar dessa volumosa soma de processos, manualmente, pode ser muito custoso para a empresa, sendo capaz de prejudicar o bem-estar financeiro e jurídico dela.

Diante desse cenário, surgiram empresas que ofertam \enquote{inteligência jurídica}, acompanhando os inúmeros processos jurídicos por meio da sua coleta, armazenando-os em um banco de dados. Esses dados processuais são disponibilizados pelos sistemas de justiça providenciados por meio das suas plataformas digitais como o Projudi do TJPE \cite{tjpe} e o e-SAJ CE \cite{esajce}. Essa coleta otimiza o trabalho das equipes jurídicas das empresas, no entanto, esses dados podem ser usados para fornecer muito mais informações para o setor jurídico da empresa e até contribuir para o seu \textit{business intelligence} (BI), revelando novas perspectivas que servirão de base para a tomada de decisão.

Mas no Brasil existem diversas plataformas digitais de justiça, em que cada uma disponibiliza os dados à sua maneira. Essa diferenciação na representação de um mesmo tipo de informação é uma problema quando se deseja usar técnicas de \textit{business intelligence} por meio de um \textit{data warehouse} (Seção \ref{subsec:datawarehouse}), pois sem um tratamento prévio, o agrupamento dos dados nessa situação não apresenta resultados satisfatórios. Então é necessário utilizar alguma estratégia para padronizar esses dados, sendo o ETL (Seção \ref{subsec:etl}) uma das técnicas mais utilizadas para tal.


\subsection{Processos}
\label{subsec:processos}

Os elementos contidos no processo são imprescindíveis para a construção do \textit{data warehouse} focado na obtenção de conhecimento jurídico e na aplicação da jurimetria, isto é, \enquote{\textit{a estatística aplicada ao Direito}} \cite{newlawJurimetria}.

Os principais dados de um processo judicial, mas não se limitando somente a eles, são a sua Numeração Processual Única (NPU) ou somente número, as partes envolvidas, os andamentos ou movimentações e a fonte.

A NPU é composta por vinte dígitos e foi instituída pelo Conselho Nacional de Justiça (CNJ) com o intuito de facilitar a consulta das informações referente a um processo, pois os números dos processos mudavam frequentemente em cada instância ou recurso. A Numeração Processual Única vale para os tribunais de todo o país e apresentam a seguinte estrutura \enquote{NNNNNNN-DD.AAAA.J.TR.OOOO} no qual \cite{jusbrasilNPU}:

\begin{itemize}
    \item \textbf{NNNNNNN-DD}: representam o número sequencial do processo e seu dígito verificador \cite{TRF4NPU}.
    \item \textbf{AAAA}: diz respeito ao ano de avaliação do processo \cite{TRF4NPU}.
    \item \textbf{J}: informa qual o órgão ou segmento do Poder Judiciário que o processo pertence \cite{TRF4NPU}.
    \item \textbf{TR}: indica o tribunal do respectivo segmento ou circunscrição judiciária \cite{TRF4NPU}.
    \item \textbf{OOOO}: remete à unidade de origem do processo \cite{TRF4NPU}.
\end{itemize}

As partes do processo são compostas pelo autor, que desempenha o papel de polo ativo do processo, ou seja, aquele que recorre à tutela jurídica do estado, tomando a posição ativa, e pelo réu, que realiza o papel do polo passivo, estando sujeito à ação processual iniciada pelo autor.

As movimentações de um processo são um conjunto de informações que comunicam a evolução processual ao longo da tramitação judicial. Elas podem conter os mais diferentes tipos de conhecimentos a respeito do processo, como os informes sobre o dia de uma audiência, notificação de emissão de intimação ou até mesmo a notícia da sentença do juiz. Nesse sentido, é usando as informações dos andamentos que o ETL fará a maior parte do seu processamento. As movimentações podem ser compostas pelos seguintes campos: 

\begin{itemize}
    \item \textbf{Título}: informa o assunto que o andamento trata.
    \item \textbf{Texto}: corpo do texto (opcional).
    \item \textbf{Complemento}: informações complementares (opcional)
    \item \textbf{Data}: dia e hora que o andamento foi disponibilizado no sistema de justiça.
\end{itemize}

Por fim, a fonte informa de maneira explícita em qual sistema o processo foi extraído. Essa informação é útil para definir quais rotinas serão aplicadas na etapa de transformação (Seção \ref{subsec:transformacao}) do ETL.


\subsection{ETL para Jurimetria}
\label{subsec:jurimetria}

A técnica de ETL usada neste trabalho segue o modelo de \textit{pipeline}, no qual cada uma das etapas do fluxo de trabalho possuem uma tarefa muito bem estabelecida e são conectadas em série (Figura \ref{fig:stepsFlow}). Cada nó dessa fila é uma estrutura que recebe o nome de \textit{step}.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{imagens/steps-flow.png}
\caption{Fluxo de execução dos \textit{steps}.}
\label{fig:stepsFlow}
\end{figure}

O \textit{step} é um conjunto de operações que serão aplicadas ao processo para realizar um objetivo bem definido. No projeto, ele é codificado como uma classe Python (Seção \ref{subsec:python}) com um método obrigatório chamado \textit{transform}. O \textit{transform} é a função que receberá o processo e aplicará as inferências e transformações necessárias para alcançar o seu objetivo.

Os processos são extraídos dos vários sistemas jurídicos pela equipe de extração e armazenados como documentos em um banco de dados MongoDB (Seção \ref{subsec:mongo}). Esses documentos serão usados no processamento do ETL, podendo ser combinados com informações complementares fornecidas pela empresa, que preparará os dados e também criará novos para serem inseridos no \textit{data warehouse}. No ETL, os documentos são lidos do MongoDB e estruturados como um dicionário do Python (Figura \ref{fig:processoPython}).

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{imagens/processo-python.png}
\caption{Exemplo de um processo como um dicionário do Python.}
\label{fig:processoPython}
\end{figure}


\subsubsection{Steps}
\label{subsec:steps}

Para esse trabalho, não serão abordados todos os \textit{steps} que são efetivamente realizados pela empresa, mas somente um recorte de alguns deles. Com exceção do primeiro \textit{step}, que trata da normalização dos dados, cada um dos demais \textit{steps} são responsáveis por adicionar mais um campo na raiz do dicionário que representa o processo (Figura \ref{fig:processoInStep}). Normalmente esse campo tem o nome do próprio \textit{step} que o gerou e seu valor é uma lista de dicionários. Esses novos campos darão origem às tabelas no banco de dados final, em que o nome da tabela será o nome do próprio campo, as suas colunas serão as chaves dos dicionários que estão na lista e os valores que popularão a tabela serão os valores dos dicionários em que a chave corresponda ao nome da coluna (Figura \ref{fig:processoParaTabela}).

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{imagens/processo-in-step.png}
\caption{Exemplo de \textit{step} enriquecendo o documento (processo) com novas informações.}
\label{fig:processoInStep}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{imagens/processo-para-tabela.png}
\caption{Conversão de novo campo inserido por um \textit{step} em tabela.}
\label{fig:processoParaTabela}
\end{figure}

O primeiro \textit{step}, chamado de \enquote{\textit{ProcessoClean}}, é o responsável pela normalização dos dados de um processo por meio de diversas conversões e substituições. Esse passo é necessário, pois as diversas informações que estão no processo são representadas como o tipo de dado \textit{string}, o que tornaria algumas das comparações difíceis e menos diretas, e por serem capturadas de diversos sistemas, um mesmo tipo de informação pode não seguir um mesmo padrão. Além disso, alguns campos de textos podem estar mal formatados e conter caracteres especiais que aumentam a complexidade das buscas necessárias para o enriquecimento dos dados. Os principais casos que esse \textit{step} trata são os tipos numéricos, as datas e os textos.

As \textit{strings} que representam algum tipo numérico como inteiros (\textit{integer}) ou números de ponto flutuante (\textit{float}) devem ser convertidas para os seus respectivos tipos, pois somente dessa forma as agregações realizadas pelo \textit{data warehouse}, como somas e médias, podem ser executadas corretamente. Para esse fim, as expressões regulares, ou \enquote{regex} (abreviação do inglês \textit{regular expression}), são uma ótima ferramenta na preparação desses textos para serem transformados, porque conseguem representar uma infinidade de padrões de maneira bem simples. Expressões como \enquote{[0-9]+} ou \enquote{\textbackslash d+} representam uma cadeia de números com 1 ou mais dígitos. Com isso, se torna muito simples a remoção de qualquer \enquote{impureza}, ou seja, tudo aquilo que não exprime um número, antes de modificar a \textit{string} para \textit{interger} ou \textit{float}.

As datas também precisam passar do formato de texto para um que reflita melhor as suas características e facilite a execução dos passos em que elas são essenciais. No nosso caso, o formato para o qual as datas em formato \textit{string} serão convertidas será o \textit{datetime} da linguagem Python e o \textit{timestamp} quando forem inseridas no banco de dados final. Com ele é possível realizar facilmente diversas operações que seriam muito mais difíceis de serem executadas sobre \textit{strings} e também padronizamos as várias formas que as datas podem apresentar (Ex: \enquote{03/11/2021}, \enquote{03-11-2021} e \enquote{2021.11.03}). Com o \textit{datetime} é possível comparar se uma data é maior que outra sem erros, ser maior significa que uma data é posterior a outra. Em um exemplo simples, colacionando duas datas em padrão de texto, em que a primeira data é \enquote{10/11/2021} e a segunda \enquote{11/10/2021}, se efetuarmos a operação de maior que entre a primeira e segunda data, obteremos o resultado falso, porque esse tipo de operação entre \textit{strings} é feita comparando carácter a carácter, porém, a primeira data é posterior a segunda, em outras palavras, o resultado esperado é verdadeiro. Além de tudo, outras operações como verificar a distância (número de dias) entre datas se torna impraticável quando elas são \textit{strings}.

E por último, será executada a normalização dos textos que já estão na tipagem correta (\textit{string}). A finalidade dessa etapa é alterar o conteúdo de modo a facilitar a pesquisa de informações sobre ele, eliminando tudo aquilo que for desnecessário e simplificando algumas informações. Inicialmente serão substituídos todos os espaços múltiplos (mais de um espaço em sequência) e outros espaçamentos especiais como tabulação, quebra de linha, quebra de página, etc, por um espaço simples. Isso facilita no momento de definir alguns padrões de busca com múltiplos termos como \enquote{decreto a sentença conforme descrito no artigo XX\dots}, tornando a escrita de uma busca exata ou de uma pesquisa com regex muito mais descomplicada. Outra padronização feita com o mesmo objetivo da transformação dos espaços extras e especiais é uniformização da \enquote{caixa das letras} dos textos, deixando todas em caixa alta ou caixa baixa, isso é feito, pois não é verdadeiro o resultado da comparação entre uma mesma letra maiúscula e minúscula. O efeito negativo que isso causa nas pesquisas pode ser exemplificado com a busca do termo \enquote{juntada de protocolo} no trecho \enquote{Juntada de protocolo de petição: cumprido em 01/05/2020.}, em que o termo não seria encontrado, visto que a letra \enquote{J} da palavra \enquote{Juntada} está em caixa diferente em ambos os textos. Existe um tratamento \enquote{especial} que deve ser feito com os textos em português também com o objetivo de facilitar as buscas. Refere-se à substituição das vogais acentuadas, como \enquote{à} e \enquote{é}, e da letra \enquote{cê-cedilha} pelos seus equivalentes na tabela ASCII (\textit{American Standard Code for Information Interchange}). O problema que a não substituição dessas letras gera é semelhante ao que ocorre com a comparação entre letras em caixas diferentes.

% TODO falar de forma geral sobre os demais steps
Os demais \textit{steps} não fazem alterações nos dados presentes no processo, mas adicionam novas informações com base em inferências feitas sobre os dados já presentes. De forma geral, as investigações executadas em cima dos dados são efetuadas usando três técnicas: uma pesquisa exata do termo, busca de padrão usando regex e o rastreio de expressões utilizando busca \textit{fuzzy}, que são buscas focadas em encontrar correspondências por aproximação, ao invés de exatidão. Com base nos resultados obtidos dessas investigações, cada \textit{step} pode introduzir novas informações ao processo, que juntamente com ele, servirá como entrada para o \textit{step} seguinte.

% TODO Escrever sobre a carga de dados no athena e postgres
Finalmente, após todos os processos passarem pelos \textit{steps}, os dados serão armazenados em formato JSON no Amazon S3 (Seção \ref{subsec:s3}). Em seguida, o Athena (Seção \ref{subsec:athena}) é utilizado para estruturar esses dados que estão em JSON como tabelas de um modelo relacional a partir de arquivos no formato CSV (\textit{Comma Separated Values}) que o próprio Athena criará. Depois disso, esses arquivos CSV são utilizados para exportar as informações para o banco de dados PostgreSQL (Seção \ref{subsec:postgres}), sendo esse o formato final pré \textit{data warehouse}. Por mais contra contra-intuitivo que isso pareça, exportar o dados diretamente do Python para o PostgreSQL oferecia um desempenho bastante abaixo do que a abordagem supracitada, isso porque a maneira mais rápida de se carregar dados no PostgreSQL é utilizando CSV, exceto por usar dados em formato binário, o que diminuiu drasticamente o tempo de carga. Concluído o armazenamento no PostgreSQL, os dados estão prontos para serem carregados em qualquer sistema ou serviços de análise de negócios como o Power Bi da Microsoft ou o Google Data Studio.